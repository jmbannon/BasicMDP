{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import torch\n",
    "from collections import namedtuple, defaultdict\n",
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiPolicy:    \n",
    "    def __init__(self):\n",
    "        # (row, column, passenger_loc, destination, action)\n",
    "        self.Q0 = torch.zeros([500, 6], dtype=torch.float32)\n",
    "        self.Q1 = None\n",
    "        self.num_actions = 6\n",
    "\n",
    "    # Behavioral action\n",
    "    def action_b(self, state):    \n",
    "        return random.randint(0, self.num_actions - 1)\n",
    "    def p_action_b(self, state, action):\n",
    "        return 1 / self.num_actions\n",
    "    \n",
    "    # Action using Q0\n",
    "    def action(self, state, epsilon=0.0, Q=None):\n",
    "        if Q is None:       \n",
    "            Q = self.Q0\n",
    "\n",
    "        action_max_q, action_max_idx = torch.max(Q[state], 0)\n",
    "        if epsilon > 0:\n",
    "            # If probability is greater than epsilon, choose a random action\n",
    "            p = random.uniform(0, 1)\n",
    "            if p > epsilon:\n",
    "                return random.randint(0, self.num_actions - 1)\n",
    "        # Otherwise choose greedy max action\n",
    "        return int(action_max_idx)\n",
    "\n",
    "    def p_action(self, state, action, epsilon=0.0, Q=None):\n",
    "        if Q is None:       \n",
    "            Q = self.Q0\n",
    "\n",
    "        action_max_q, action_max_idx = torch.max(Q[state], 0)\n",
    "        if action == action_max_idx:\n",
    "            return epsilon + ((1 - epsilon) / self.num_actions)\n",
    "        else:\n",
    "            return (1 - epsilon) / self.num_actions\n",
    "        \n",
    "    \n",
    "    # Action using Q0 and Q1\n",
    "    def action_d(self, state, epsilon):\n",
    "        action_max_q, action_max_idx = torch.max(self.Q0[state] + self.Q1[state], 0)\n",
    "        if epsilon > 0:\n",
    "            # If probability is greater than epsilon, choose a random action\n",
    "            p = random.uniform(0, 1)\n",
    "            if p > epsilon:\n",
    "                return random.randint(0, self.num_actions - 1)\n",
    "        # Otherwise choose greedy max action\n",
    "        return int(action_max_idx)\n",
    "    \n",
    "    # Trains single-step TD-learning algorithms\n",
    "    #   algo = {'sarsa', 'q-learning', 'expected-sarsa', 'double-q-learning'\n",
    "    def train(self, env, iterations, epsilon, learning_rate, discount, algo):\n",
    "        if algo == 'double-q-learning':\n",
    "            self.Q1 = torch.zeros([500, 6], dtype=torch.float32)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            state = env.reset()\n",
    "            q_update = 0\n",
    "            a_ = None\n",
    "            ep_ended = False\n",
    "            while not ep_ended:\n",
    "                # If next action is not defined, take action based on current state\n",
    "                action = a_ if a_ else self.action(state, epsilon)\n",
    "                \n",
    "                # (state', reward, ep_ended, prob=1.0)\n",
    "                s_, r, ep_ended, prob = env.step(action)\n",
    "                \n",
    "                if algo == 'sarsa':\n",
    "                    # Uses next action\n",
    "                    a_ = self.action(s_, epsilon)\n",
    "                    update = self.Q0[s_, a_]\n",
    "                elif algo == 'q-learning':\n",
    "                    update = self.Q0[s_, self.action(s_)]\n",
    "                elif algo == 'expected-sarsa':\n",
    "                    # Gives all actions (1 - epsilon) / num_action probabilities\n",
    "                    next_action_probs = torch.ones([self.num_actions]).fill_((1 - epsilon) / self.num_actions)\n",
    "                    \n",
    "                    # Adds epsilon probability to best action\n",
    "                    next_action_probs[self.action(s_)] += epsilon\n",
    "                    \n",
    "                    update = torch.sum(next_action_probs * self.Q0[s_, :])\n",
    "                elif algo == 'double-q-learning':\n",
    "                    # Next action uses both Q's\n",
    "                    a_ = self.action_d(s_, epsilon)\n",
    "                    \n",
    "                    # Updates only one of the Q's using the other Q\n",
    "                    q_update = random.randint(0, 2)\n",
    "                    if q_update == 0:\n",
    "                        update = self.Q1[s_, self.action(s_, Q=self.Q0)]\n",
    "                    else:\n",
    "                        update = self.Q0[s_, self.action(s_, Q=self.Q1)]  \n",
    "                else:\n",
    "                    raise Exception('Invalid algo')\n",
    "                    \n",
    "                if q_update == 0:\n",
    "                    self.Q0[state, action] += learning_rate * (r + (discount * update) - self.Q0[state, action])\n",
    "                else:\n",
    "                    self.Q1[state, action] += learning_rate * (r + (discount * update) - self.Q1[state, action])\n",
    "\n",
    "                state = s_\n",
    "\n",
    "    # Trains n-step TD learning. We keep track of TD-Error buffers for sigma={0,1}. Our approach differs\n",
    "    # from the algorithm in Sutton by setting sigma_t equal w.r.t the backup diagram time step versus\n",
    "    # any arbitrary time.\n",
    "    #   NOTE: Off_policy with taxi environment is difficult to train due to the behavioral policy randomly\n",
    "    #   stepping in the grid world.\n",
    "    def train_nstep(self, env, iterations, epsilon, learning_rate, discount, n, nsigma, off_policy=False):\n",
    "        assert len(nsigma) == n, 'nsigma must have n values of {0, 1}'\n",
    "        bn = n + 1   # Buffers needs to hold n+1 values\n",
    "        \n",
    "        for ep in range(iterations):\n",
    "            sbuffer = torch.zeros([bn], dtype=torch.int64)      # state buffer\n",
    "            abuffer = torch.zeros([bn], dtype=torch.int64)      # action buffer\n",
    "            qbuffer = torch.zeros([bn], dtype=torch.float32)    # state-action (Q) value buffer\n",
    "            tdbuffer0 = torch.zeros([bn], dtype=torch.float32)  # tderror buffer (sigma=0)\n",
    "            tdbuffer1 = torch.zeros([bn], dtype=torch.float32)  # tderror buffer (sigma=1)\n",
    "            pbuffer = torch.zeros([bn], dtype=torch.float32)    # state-action prob buffer\n",
    "            rbuffer = torch.zeros([bn], dtype=torch.float32)    # importance sampling ratio buffer\n",
    "            \n",
    "            T = np.inf     # End of episode time\n",
    "            t = 0          # Current time in episode\n",
    "            tau = 0        # Time whose estimate is being updated\n",
    "\n",
    "            state = env.reset()\n",
    "            if off_policy:\n",
    "                action = self.action_b(state)\n",
    "            else:\n",
    "                action = self.action(state, epsilon)\n",
    "            \n",
    "            # Update initial buffers\n",
    "            sbuffer[t % bn] = int(state)\n",
    "            abuffer[t % bn] = int(action)\n",
    "            qbuffer[t % bn] = self.Q0[state, action]\n",
    "\n",
    "            while t < T:\n",
    "                if t < T:\n",
    "                    # (state', reward, ep_ended, prob=1.0)\n",
    "                    state, r, ep_ended, prob = env.step(action)\n",
    "                    sbuffer[(t + 1) % bn] = int(state)\n",
    "                                        \n",
    "                    if ep_ended:\n",
    "                        T = t + 1\n",
    "                        tdbuffer0[t % bn] = r - qbuffer[t % bn]\n",
    "                        tdbuffer1[t % bn] = r - qbuffer[t % bn]\n",
    "                    else:\n",
    "                        if off_policy:\n",
    "                            action = self.action_b(state)\n",
    "                        else:\n",
    "                            action = self.action(state, epsilon)\n",
    "                        \n",
    "                        abuffer[(t + 1) % bn] = int(action)\n",
    "                        qbuffer[(t + 1) % bn] = self.Q0[state, action]\n",
    "\n",
    "                        # For expected update sigma=0\n",
    "                        # Gives all actions (1 - epsilon) / num_action probabilities\n",
    "                        next_action_probs = torch.ones([self.num_actions]).fill_((1 - epsilon) / self.num_actions)\n",
    "\n",
    "                        # Adds epsilon probability to best action\n",
    "                        next_action_probs[self.action(state)] += epsilon\n",
    "\n",
    "                        tdbuffer0[t % bn] = r + (discount * torch.sum(next_action_probs * self.Q0[state, :])) - qbuffer[t % bn]\n",
    "\n",
    "                        # For td update sigma=1\n",
    "                        tdbuffer1[t % bn] = r + (discount * qbuffer[(t + 1) % bn]) - qbuffer[t % bn]\n",
    "\n",
    "                        # Update probability and importance sampling ratio buffer\n",
    "                        pbuffer[(t + 1) % bn] = self.p_action(state, action, epsilon)\n",
    "                        rbuffer[(t + 1) % bn] = pbuffer[(t + 1) % bn] / self.p_action_b(state, action)\n",
    "\n",
    "                # tau being the timestep to update\n",
    "                tau = t - n + 1\n",
    "                if tau >= 0: \n",
    "                    p = 1\n",
    "                    z = 1\n",
    "                    G = qbuffer[tau % bn]\n",
    "\n",
    "                    for i,k in enumerate(range(tau, min(tau + n, T))):\n",
    "                        # Update reward w.r.t the timestep of the backup diagram\n",
    "                        if nsigma[i] == 0:\n",
    "                            G += (z * tdbuffer0[k % bn])\n",
    "                        else:\n",
    "                            G += (z * tdbuffer1[k % bn])\n",
    "\n",
    "                        z *= (discount * (((1 - nsigma[(i + 1) % n]) * pbuffer[(k + 1) % bn]) + nsigma[(i + 1) % n]))\n",
    "                        p *= (1 - nsigma[i % n] + (nsigma[i % n] * rbuffer[k % bn]))\n",
    "\n",
    "                    # If off_policy, use importance sampling ratio. Otherwise, exclude it.\n",
    "                    if off_policy:\n",
    "                        self.Q0[sbuffer[tau % bn], abuffer[tau % bn]] += (learning_rate * p * (G - self.Q0[sbuffer[tau % bn], abuffer[tau % bn]]))\n",
    "                    else:\n",
    "                        self.Q0[sbuffer[tau % bn], abuffer[tau % bn]] += (learning_rate * (G - self.Q0[sbuffer[tau % bn], abuffer[tau % bn]]))\n",
    "                t += 1\n",
    "\n",
    "    def play(self, env):\n",
    "        state = env.reset()\n",
    "        ep_ended = False\n",
    "                \n",
    "        env.render()\n",
    "        while not ep_ended:\n",
    "            if self.Q1 is None:\n",
    "                action = self.action(state, epsilon=0)\n",
    "            else:\n",
    "                action = self.action_d(state, epsilon=0)\n",
    "\n",
    "            s_, r, ep_ended, prob = env.step(action)\n",
    "            state = s_\n",
    "            \n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TaxiPolicy()\n",
    "t.train_nstep(env,\n",
    "              iterations=10000,\n",
    "              epsilon=0.9,\n",
    "              learning_rate=0.1,\n",
    "              discount=0.9,\n",
    "              n=8,\n",
    "              nsigma=[1, 1, 1, 1, 1, 1, 1, 0], \n",
    "              off_policy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [-11.4065, -11.0614, -10.9875, -13.8998,  -8.1963, -18.8504],\n",
      "        [-13.6035, -13.5010, -13.1343, -13.6932,  -7.0112, -18.8182],\n",
      "        ...,\n",
      "        [ -5.0446,  -6.0407,  -9.8037,  -7.4085, -17.9992, -14.2536],\n",
      "        [-13.9695, -13.3160, -14.1885, -11.8464, -23.0467, -25.4842],\n",
      "        [  0.0000,   0.0000,  -2.9619,   0.0000,  -4.4078,  -3.8619]])\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(t.Q0)\n",
    "t.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
