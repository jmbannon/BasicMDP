{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import torch\n",
    "from collections import namedtuple, defaultdict\n",
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiPolicy:    \n",
    "    def __init__(self):\n",
    "        # (row, column, passenger_loc, destination, action)\n",
    "        self.Q0 = torch.zeros([500, 6], dtype=torch.float32)\n",
    "        self.Q1 = None\n",
    "        self.num_actions = 6\n",
    "\n",
    "    def action(self, state, epsilon=0.0, Q=None):\n",
    "        if Q is None:       \n",
    "            Q = self.Q0\n",
    "\n",
    "        action_max_q, action_max_idx = torch.max(Q[state], 0)\n",
    "        if epsilon > 0:\n",
    "            # If probability is greater than epsilon, choose a random action\n",
    "            p = random.uniform(0, 1)\n",
    "            if p > epsilon:\n",
    "                return random.randint(0, self.num_actions - 1)\n",
    "        # Otherwise choose greedy max action\n",
    "        return int(action_max_idx)\n",
    "    \n",
    "    def action_d(self, state, epsilon):\n",
    "        action_max_q, action_max_idx = torch.max(self.Q0[state] + self.Q1[state], 0)\n",
    "        if epsilon > 0:\n",
    "            # If probability is greater than epsilon, choose a random action\n",
    "            p = random.uniform(0, 1)\n",
    "            if p > epsilon:\n",
    "                return random.randint(0, self.num_actions - 1)\n",
    "        # Otherwise choose greedy max action\n",
    "        return int(action_max_idx)\n",
    "        \n",
    "    def train(self, env, iterations, epsilon, learning_rate, discount, algo):\n",
    "        if algo == 'double-q-learning':\n",
    "            self.Q1 = torch.zeros([500, 6], dtype=torch.float32)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            state = env.reset()\n",
    "            q_update = 0\n",
    "            a_ = None\n",
    "            ep_ended = False\n",
    "            while not ep_ended:\n",
    "                # If next action is not defined, take action based on current state\n",
    "                action = a_ if a_ else self.action(state, epsilon)\n",
    "                \n",
    "                # (state', reward, ep_ended, prob=1.0)\n",
    "                s_, r, ep_ended, prob = env.step(action)\n",
    "                \n",
    "                if algo == 'sarsa':\n",
    "                    # Uses next action\n",
    "                    a_ = self.action(s_, epsilon)\n",
    "                    update = self.Q0[s_, a_]\n",
    "                elif algo == 'q-learning':\n",
    "                    update = self.Q0[s_, self.action(s_)]\n",
    "                elif algo == 'expected-sarsa':\n",
    "                    # Gives all actions (1 - epsilon) / num_action probabilities\n",
    "                    next_action_probs = torch.ones([self.num_actions]).fill_((1 - epsilon) / self.num_actions)\n",
    "                    \n",
    "                    # Adds epsilon probability to best action\n",
    "                    next_action_probs[self.action(s_)] += epsilon\n",
    "                    \n",
    "                    update = torch.sum(next_action_probs * self.Q0[s_, :])\n",
    "                elif algo == 'double-q-learning':\n",
    "                    # Next action uses both Q's\n",
    "                    a_ = self.action_d(s_, epsilon)\n",
    "                    \n",
    "                    # Updates only one of the Q's using the other Q\n",
    "                    q_update = random.randint(0, 2)\n",
    "                    if q_update == 0:\n",
    "                        update = self.Q1[s_, self.action(s_, Q=self.Q0)]\n",
    "                    else:\n",
    "                        update = self.Q0[s_, self.action(s_, Q=self.Q1)]  \n",
    "                else:\n",
    "                    raise Exception('Invalid algo')\n",
    "                    \n",
    "                if q_update == 0:\n",
    "                    self.Q0[state, action] += learning_rate * (r + (discount * update) - self.Q0[state, action])\n",
    "                else:\n",
    "                    self.Q1[state, action] += learning_rate * (r + (discount * update) - self.Q1[state, action])\n",
    "\n",
    "                state = s_\n",
    "\n",
    "    def play(self, env):\n",
    "        state = env.reset()\n",
    "        ep_ended = False\n",
    "                \n",
    "        env.render()\n",
    "        while not ep_ended:\n",
    "            if self.Q1 is None:\n",
    "                action = self.action(state, epsilon=0)\n",
    "            else:\n",
    "                action = self.action_d(state, epsilon=0)\n",
    "\n",
    "            s_, r, ep_ended, prob = env.step(action)\n",
    "            state = s_\n",
    "            \n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TaxiPolicy()\n",
    "t.train(env, iterations=10000, epsilon=0.9, learning_rate=0.1, discount=0.9, algo='double-q-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00],\n",
      "        [-2.8650e+00, -1.7954e+00, -2.0245e+00, -1.2941e+00,  4.3489e+00,\n",
      "         -5.7377e+00],\n",
      "        [-1.5886e+00,  1.9355e-01, -2.9001e-01, -1.3402e+00,  7.7147e+00,\n",
      "         -5.5879e+00],\n",
      "        ...,\n",
      "        [-8.7840e-01, -2.4084e-01, -1.0437e+00, -9.2251e-01, -2.0507e+00,\n",
      "         -2.9247e+00],\n",
      "        [-2.3107e+00, -2.3559e+00, -2.4237e+00,  1.2643e-02, -3.5358e+00,\n",
      "         -5.1121e+00],\n",
      "        [ 0.0000e+00, -1.4770e-01, -2.7100e-01,  1.4424e+01, -1.1710e+00,\n",
      "         -1.0060e+00]])\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(t.Q0)\n",
    "t.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
