{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import torch\n",
    "from collections import namedtuple, defaultdict\n",
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiPolicy:    \n",
    "    def __init__(self):\n",
    "        # (row, column, passenger_loc, destination, action)\n",
    "        self.Q = torch.zeros([500, 6], dtype=torch.float32)\n",
    "        self.num_actions = 6\n",
    "\n",
    "    def action(self, state, epsilon=0.0, Q=None):\n",
    "        if Q is None:\n",
    "            Q = self.Q\n",
    "    \n",
    "        action_max_q, action_max_idx = torch.max(Q[state], 0)\n",
    "        if epsilon > 0:\n",
    "            # If probability is greater than epsilon, choose a random action\n",
    "            p = random.uniform(0, 1)\n",
    "            if p > epsilon:\n",
    "                return random.randint(0, self.num_actions - 1)\n",
    "        # Otherwise choose greedy max action\n",
    "        return int(action_max_idx)\n",
    "    \n",
    "    def sarsa_update(self, state, action, s_, r):\n",
    "        return learning_rate * (r + (discount * self.Q[s_, self.action(s_)]) - self.Q[state, action])\n",
    "        \n",
    "    def train(self, env, iterations, epsilon, learning_rate, discount, algo):\n",
    "        if algo == 'double-q-learning':\n",
    "            Q2 = torch.zeros([500, 6], dtype=torch.float32)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            state = env.reset()\n",
    "            a_ = None\n",
    "            ep_ended = False\n",
    "            while not ep_ended:\n",
    "                # If next action is not defined, take action based on current state\n",
    "                action = a_ if a_ else self.action(state, epsilon)\n",
    "                \n",
    "                # (state', reward, ep_ended, prob=1.0)\n",
    "                s_, r, ep_ended, prob = env.step(action)\n",
    "                \n",
    "                if algo == 'sarsa':\n",
    "                    # Uses next action\n",
    "                    a_ = self.action(s_, epsilon)\n",
    "                    update = self.Q[s_, a_]\n",
    "                elif algo == 'q-learning':\n",
    "                    update = self.Q[s_, self.action(s_)]\n",
    "                elif algo == 'expected-sarsa':\n",
    "                    # Gives all actions (1 - epsilon) / num_action probabilities\n",
    "                    next_action_probs = torch.ones([self.num_actions]).fill_((1 - epsilon) / self.num_actions)\n",
    "                    \n",
    "                    # Adds epsilon probability to best action\n",
    "                    next_action_probs[self.action(s_)] += epsilon\n",
    "                    \n",
    "                    update = torch.sum(next_action_probs * self.Q[s_, :])\n",
    "                elif algo == 'double-q-learning'\n",
    "                    q_update = random.randint(0, 2)\n",
    "                    if q_update == 0:\n",
    "                        update = Q2[s_, self.action(s_, Q=self.Q)]\n",
    "                    else:\n",
    "                        update = self.Q[s_, self.action(s_, Q=Q2)]\n",
    "                    \n",
    "                else:\n",
    "                    raise Exception('Invalid algo')\n",
    "                    \n",
    "                self.Q[state, action] += learning_rate * (r + (discount * update) - self.Q[state, action])\n",
    "                state = s_\n",
    "\n",
    "    def play(self, env):\n",
    "        state = env.reset()\n",
    "        ep_ended = False\n",
    "                \n",
    "        env.render()\n",
    "        while not ep_ended:\n",
    "            action = self.action(state, epsilon=0)\n",
    "            s_, r, ep_ended, prob = env.step(action)\n",
    "            state = s_\n",
    "            \n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TaxiPolicy()\n",
    "t.train(env, iterations=10000, epsilon=0.9, learning_rate=0.1, discount=0.9, algo='expected-sarsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.6579, -0.7136, -1.6771, -0.1949,  1.6689, -8.0344],\n",
      "        [ 0.8803,  2.2542, -0.1101,  2.2944,  5.4320, -5.2717],\n",
      "        ...,\n",
      "        [-1.2585,  4.4897, -1.1682, -1.2324, -1.9044, -1.9918],\n",
      "        [-2.5955, -2.5871, -2.5888, -0.6030, -3.8501, -3.7924],\n",
      "        [ 0.5403,  0.8449, -0.1032, 15.9046, -1.0015, -1.0000]])\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(t.Q)\n",
    "t.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
