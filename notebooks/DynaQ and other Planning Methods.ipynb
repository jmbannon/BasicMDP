{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import torch\n",
    "from collections import namedtuple, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiPolicy:    \n",
    "    def __init__(self):\n",
    "        # (row, column, passenger_loc, destination, action)\n",
    "        self.Q = torch.zeros([500, 6], dtype=torch.float32)\n",
    "        self.Qmem = {}\n",
    "        self.num_actions = 6\n",
    "\n",
    "    # Behavioral action\n",
    "    def action_b(self, state):\n",
    "        \"\"\"Chooses random behavioral action.\"\"\"\n",
    "        return random.randint(0, self.num_actions - 1)\n",
    "    def p_action_b(self, state, action):\n",
    "        \"\"\"Returns random behavioral action probability\"\"\"\n",
    "        return 1 / self.num_actions\n",
    "    \n",
    "    # Action using Q0\n",
    "    def action(self, state, epsilon=0.0, Q=None):\n",
    "        action_max_q, action_max_idx = torch.max(self.Q[state], 0)\n",
    "        if epsilon > 0:\n",
    "            # If probability is greater than epsilon, choose a random action\n",
    "            p = random.uniform(0, 1)\n",
    "            if p > epsilon:\n",
    "                return random.randint(0, self.num_actions - 1)\n",
    "        # Otherwise choose greedy max action\n",
    "        return int(action_max_idx)\n",
    "\n",
    "    def p_action(self, state, action, epsilon=0.0):\n",
    "        action_max_q, action_max_idx = torch.max(self.Q[state], 0)\n",
    "        if action == action_max_idx:\n",
    "            return epsilon + ((1 - epsilon) / self.num_actions)\n",
    "        else:\n",
    "            return (1 - epsilon) / self.num_actions\n",
    "    \n",
    "    # Trains DynaQ\n",
    "    def train(self, env, iterations, epsilon, learning_rate, discount, planning_iterations):\n",
    "        for i in range(iterations):\n",
    "            state = env.reset()\n",
    "            q_update = 0\n",
    "            a_ = None\n",
    "            ep_ended = False\n",
    "            while not ep_ended:\n",
    "                # If next action is not defined, take action based on current state\n",
    "                action = a_ if a_ else self.action(state, epsilon)\n",
    "                \n",
    "                # (state', reward, ep_ended, prob=1.0)\n",
    "                s_, r, ep_ended, prob = env.step(action)\n",
    "                \n",
    "                # Update memory for planning\n",
    "                if state not in self.Qmem:\n",
    "                    self.Qmem[state] = {}\n",
    "                self.Qmem[state][action] = (s_, r)\n",
    "                \n",
    "                # Expected reward from next best action\n",
    "                update = self.Q[s_, self.action(s_)]\n",
    "                \n",
    "                self.Q[state, action] += learning_rate * (r + (discount * update) - self.Q[state, action])\n",
    "                \n",
    "                state = s_\n",
    "                \n",
    "                for _ in range(planning_iterations):\n",
    "                    s__ = random.choice(list(self.Qmem.keys()))\n",
    "                    a__ = random.choice(list(self.Qmem[s__].keys()))\n",
    "                    s___, r = self.Qmem[s__][a__]\n",
    "                    \n",
    "                    update = self.Q[s___, self.action(s___)]\n",
    "                    \n",
    "                    self.Q[s__, a__] += learning_rate * (r + (discount * update) - self.Q[s__, a__])\n",
    "\n",
    "    def play(self, env):\n",
    "        state = env.reset()\n",
    "        ep_ended = False\n",
    "                \n",
    "        env.render()\n",
    "        while not ep_ended:\n",
    "            action = self.action(state, epsilon=0)\n",
    "            s_, r, ep_ended, prob = env.step(action)\n",
    "            state = s_\n",
    "            \n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TaxiPolicy()\n",
    "t.train(env,\n",
    "        iterations=1000,\n",
    "        epsilon=0.9,\n",
    "        learning_rate=0.1,\n",
    "        discount=0.9,\n",
    "        planning_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.5661,  2.9035,  1.5471,  2.8889,  4.3486, -6.1194],\n",
      "        [ 4.2854,  5.9018,  4.2842,  5.9250,  7.7146, -3.0856],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  2.9131, -7.3813, -7.3806],\n",
      "        [ 1.4466,  2.7467,  1.4710,  2.8746, -7.4487, -7.4995],\n",
      "        [14.2848, 11.8332, 14.2871, 16.9997,  5.2838,  5.2972]])\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "print(t.Q)\n",
    "t.play(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
