{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "import gym\n",
    "import torch\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "seed = 1412343214"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "This notebook recreates the classic card game Blackjack and trains an agent to play effectively using Monte Carlo Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ddc5b70>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "env.seed(seed)  \n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['hand', 'dealer', 'usable_ace'])\n",
    "StateAction = namedtuple('StateAction', ['s', 'a'])\n",
    "StateActionReward = namedtuple('StateActionReward', ['s', 'a', 'r'])\n",
    "Outcome = namedtuple('Outcome', ['state', 'reward', 'is_over', 'empty'])\n",
    "\n",
    "class BlackJackPolicy:\n",
    "    def __init__(self):\n",
    "        # state: [(action, prob), (action, prob)]\n",
    "        self.p = defaultdict(lambda: [(True, 0.5), (False, 0.5)])\n",
    "    def action(self, state, greedy=False):\n",
    "        if greedy:\n",
    "            return max(self.p[state], key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            actions = [a[0] for a in self.p[state]]\n",
    "            probabilities = [p[1] for p in self.p[state]]\n",
    "            return random.choices(actions, probabilities)[0] # Weighted action\n",
    "\n",
    "def episode(policy):\n",
    "    env.reset()\n",
    "    finished = False\n",
    "    state_action_pairs = []\n",
    "    while not finished:\n",
    "        state = State(*env._get_obs())\n",
    "        if state.hand >= 21 or state.dealer >= 21:\n",
    "            break\n",
    "        action = policy.action(state)\n",
    "        outcome = Outcome(*env.step(policy.action(state)))\n",
    "        \n",
    "        state_action_pairs.append(StateActionReward(state, action, outcome.reward))\n",
    "        finished = outcome.is_over\n",
    "    return state_action_pairs\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Policy First-Visit Monte Carlo Control (for epsilon-soft policies)\n",
    "Estimates optimal policy by generating episodes from the starting state. 'On Policy' refers to using the same policy to guide actions and generate behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epsilon=0.1):\n",
    "    policy = BlackJackPolicy()\n",
    "    Q = defaultdict(lambda: defaultdict(lambda: (0, 0)))  # state.action.(sum, count) of post rewards\n",
    "    \n",
    "    for i in range(10000):\n",
    "        ep_state_actions = episode(policy)                            # generate an episode\n",
    "        for i,sar in enumerate(ep_state_actions):                     # iterate state-action-rewards in episode\n",
    "            s = sar.s\n",
    "            a = sar.a\n",
    "            returns = sum(map(lambda x: x.r, ep_state_actions[i:]))   # sum of rewards that follows first occurrence\n",
    "            Q[s][a] = ((Q[s][a][0] + returns), Q[s][a][1] + 1)        # update Q(s,a) sum of rewards and count\n",
    "        for s in map(lambda sar: sar.s, ep_state_actions):\n",
    "            a_star = max(Q[s], key=lambda a: Q[s][a][0] / Q[s][a][1]) # argmax action based on Q average\n",
    "            p_a = 1 - epsilon + (epsilon / 2)                         # probability of best action\n",
    "            p_na = epsilon / 2                                        # probability of all other actions (only 1 other)\n",
    "            policy.p[s] = [(a_star, p_a), (not a_star, p_na)]         # update policy probability of action based on state\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_prediction(o_policy):\n",
    "    b_policy = BlackJackPolicy(greedy=False)\n",
    "    Q = defaultdict(lambda: defaultdict(lambda: (0, 0)))  # state.action.(sum, count) of post rewards\n",
    "    \n",
    "    for i in range(10000):\n",
    "        ep_state_actions = episode(b_policy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
